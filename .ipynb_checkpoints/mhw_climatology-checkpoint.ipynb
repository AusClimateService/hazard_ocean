{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5874-6884-4a94-a83a-317981fba5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "### data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import scipy as sci\n",
    "import numpy_groupies as npg \n",
    "\n",
    "### plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### flox for GroupBy Reductions\n",
    "import flox.xarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f8580-f353-4b4d-b331-8da75cdaba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "import dask\n",
    "client = Client(threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9c2f2-c111-48c5-bfc8-251741b0ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesser to drop unwanted variables\n",
    "def drop_stuff(ds, coords_to_drop,vars_to_drop):\n",
    "    \"\"\"\n",
    "    Preprocessor function to drop specified coordinates and variables from a dataset loaded via xr.open_mfdataset\n",
    "\n",
    "    Parameters:\n",
    "        ds (xarray.Dataset): The dataset from which coordinates & variables are to be dropped.\n",
    "        coords_to_drop (list of str): List of coordinate names to drop.\n",
    "        vars_to_drop(list of str): List of variable names to drop\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: Dataset with specified coordinates and variables dropped.\n",
    "    \"\"\"\n",
    "    # Drop coordinates if they are in the dataset\n",
    "    ds = ds.drop_vars(coords_to_drop, errors='ignore')\n",
    "    ds = ds.drop_vars(vars_to_drop, errors='ignore')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8bb84-5596-46d2-8b86-169587a7b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunks(data_array):\n",
    "    ''' \n",
    "    Print the chunk sizes for each dimension in an Xarray dataset\n",
    "    \n",
    "    Parameters: \n",
    "    data_array (xarray.Daraset): The dataset from which to print chunks\n",
    "    '''\n",
    "    # Get chunk size\n",
    "    chunks = data_array.chunks\n",
    "    # Get dimension names\n",
    "    dim_names = data_array.dims\n",
    "    \n",
    "    readable_chunks = {dim: chunks[i] for i, dim in enumerate(dim_names)}\n",
    "    \n",
    "    # Print chunk sizes for each dimension\n",
    "    for dim, sizes in readable_chunks.items():\n",
    "        print(f\"{dim} chunks: {sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5f2eb-5ebb-4ca8-8b93-df2276ac8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkdir = \"/g/data/fp2/OFAM\" #not using os.chdir() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194a244-9e5d-4bdd-9724-16324207f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define coordinates and variables to drop\n",
    "coords_to_drop =['st_edges_ocean','nv']\n",
    "vars_to_drop =['Time_bounds','average_DT','average_T1','average_T2']\n",
    "\n",
    "# Load the datasets with preprocessing\n",
    "big_sst = xr.open_mfdataset(\n",
    "    [wrkdir, \"/jra55_historical.1/surface/ocean_temp_sfc_*.nc\"], \n",
    "    parallel=True, \n",
    "    preprocess = lambda x: drop_stuff(x, \n",
    "                                         coords_to_drop, \n",
    "                                         vars_to_drop)).squeeze() #combine='by_coords' is default\n",
    "\n",
    "# Rename Time to time\n",
    "big_sst = big_sst.rename({'Time':'time'})\n",
    "\n",
    "big_sst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8edd17e-3bce-4e48-bcac-ea5a402c9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should this be assigned as a new coordinate? \n",
    "\n",
    "big_sst['doy'] = big_sst['time'].dt.dayofyear\n",
    "doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425aba2b-de35-4ff9-93cc-d08582a6db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataset around the Australian continent\n",
    "sst_reduced = big_sst.isel(yt_ocean=slice(y1,y2), \n",
    "                            xt_ocean=slice(x1,x2)).drop_vars('st_ocean')\n",
    "sst_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7007b7-dc84-4093-8dbb-eb7eec56cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the subset by DOY and take the mean using the cohort method, no chunking \n",
    "climatology_cohorts = flox.xarray.xarray_reduce(\n",
    "    sst_reduced,\n",
    "    doy,\n",
    "    func=\"mean\",\n",
    "    method=\"cohorts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238181e-0daa-49b5-97b6-22ac6c6eea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#XXL-mem normal queue\n",
    "climatology_cohorts = climatology_cohorts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a5fd5-02da-4a4e-b224-007e1112559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate mean and quantile\n",
    "def calculate_stats(chunk):\n",
    "    mean = chunk.groupby('doy').mean(dim='time')\n",
    "    thresh = chunk.groupby('doy').quantile(0.9, dim='time', skipna=True)\n",
    "    return mean, thresh\n",
    "\n",
    "# Map function across chunks using Dask delayed\n",
    "seas_list = []\n",
    "thresh_list = []\n",
    "for ii in xt_chunks:\n",
    "    for jj in yt_chunks:\n",
    "        chunk = sst_chunked.isel(xt_ocean=slice(ii, ii+di), yt_ocean=slice(jj, jj+dj))\n",
    "        mean, thresh = dask.delayed(calculate_stats)(chunk)\n",
    "        seas_list.append(mean)\n",
    "        thresh_list.append(thresh)\n",
    "\n",
    "# Compute results\n",
    "seas_results, thresh_results = dask.compute(seas_list, thresh_list)\n",
    "\n",
    "# Merge the results into single xarrays\n",
    "seas_new = xr.merge(seas_results)\n",
    "thresh_new = xr.merge(thresh_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1a753-08a3-41e3-b515-c363961e98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad and apply rolling mean window of size 31 along DOY\n",
    "climatology = climatology_cohorts.pad(doy=(31-1)//2, mode='wrap').rolling(doy=31, center=True).mean()\n",
    "threshold90 = threshold_cohorts.pad(doy=(31-1)//2, mode='wrap').rolling(doy=31, center=True).mean(skipna=True)\n",
    "\n",
    "# Chunk the data and select a subset along DOY\n",
    "climatology = climatology_cohorts.chunk({'doy':-1, 'yt_ocean':50, 'xt_ocean':50}).isel(doy=slice(15,-15))\n",
    "threshold90 = threshold_cohorts.chunk({'doy':-1, 'yt_ocean':50, 'xt_ocean':50}).isel(doy=slice(15,-15)).drop_vars('quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ff4b1-f8a8-40f7-9a9a-8feb3349e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.chdir(\"/g/data/xv83/users/ep5799/heatwaves\")\n",
    "os.getcwd()\n",
    "\n",
    "print(\"Saving climatology and threshold to disk\")\n",
    "climatology.to_netcdf('Australian_SST_daily_climatology.nc', mode='w')\n",
    "threshold90.to_netcdf('Australian_SST_daily_MHWthreshold.nc', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
